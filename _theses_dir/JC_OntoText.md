---
title: 'Ontology Text Alignment Using Large Language Models (BERT and Generative LLMS)'
layout: default
description: 'This is a qualitative study where the goal is to look into common theories and disciplines that might influence novel Hybrid Intelligence methods.'
topic: 'Knowledge Representation and Knowledge Extraction'
keywords: 
    - 'Large Language Models' 
    - 'Ontology Engineering'
    - 'Nature Langue Processing'
    - 'BERT models' 
    - 'RAG'
supervisor: 'Jieying Chen'
contact: 'j.chen2@vu.nl'
degree: 'B.Sc./M.Sc.'
description_link: '/theses_dir/JC_OntoText'
---

<!-- The informtation below doesnÂ´t need to be adjusted. It is automatically pulled from the frontmatter-->
## {{page.title}} 
*Supervisor: {{page.supervisor}} ({{page.contact}})*

### Background: 
Large Language Models (LLMs) excel in many NLP tasks but struggle with reasoning and explainability, critical in domains like biomedical and geological sciences. Ontologies, known for their reasoning capabilities, remain essential, but aligning unstructured text with ontological axioms is complex and labor-intensive.
This project tackles the problem of ontology text alignment by integrating BERT models and generative LLMs into a Retrieval-Augmented Generation (RAG) framework. With semantic enhancement via atomic decomposition, we aim to streamline the alignment process. Initial benchmarks in geology and biomedicine show promising results.

### Objectives
Develop and evaluate a RAG-based framework for ontology text alignment, focusing on:
1. Combining BERT for retrieval and generative LLMs for alignment.
2. Enhancing with atomic decomposition for improved reasoning.
3. Testing on benchmarks to assess performance.

### Expected outcomes: 
A functional framework, or/and benchmark and performance comparison with existing methods. (modificato) 

### Other projects
For more project descriptions, please check here: https://docs.google.com/document/d/1S8JdCk_Re0F189RaBjadVd8cEQwZ9sglwOMZoZioOQ0/edit?usp=sharing

